{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":63741,"databundleVersionId":6966274,"sourceType":"competition"}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-15T17:26:08.380426Z","iopub.execute_input":"2023-12-15T17:26:08.380892Z","iopub.status.idle":"2023-12-15T17:26:08.824759Z","shell.execute_reply.started":"2023-12-15T17:26:08.380853Z","shell.execute_reply":"2023-12-15T17:26:08.823852Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/book-rating-prediction01/sample_submission.csv\n/kaggle/input/book-rating-prediction01/Train.csv\n/kaggle/input/book-rating-prediction01/Test.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### 1. Loading the Dataset","metadata":{}},{"cell_type":"code","source":"dataset = pd.read_csv(\"/kaggle/input/book-rating-prediction01/Train.csv\",encoding=\"latin\")\ndf = pd.DataFrame(dataset)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T17:26:11.255465Z","iopub.execute_input":"2023-12-15T17:26:11.256007Z","iopub.status.idle":"2023-12-15T17:26:34.105183Z","shell.execute_reply.started":"2023-12-15T17:26:11.255971Z","shell.execute_reply":"2023-12-15T17:26:34.103996Z"},"trusted":true},"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"                            user_id   book_id  \\\n0  e8730ce3ed5e9762038d160e23d47d79      2187   \n1  26317e667d9141ad245dd2c18be52d77     47212   \n2  bf9640b81a047ee70b4f918082492f1d     40483   \n3  34aa99d428ad98679c3e45d117243f55  19095025   \n4  6b3f929609c9d97628807d13b59b0b22   9464746   \n\n                          review_id  \\\n0  1c6af3913167e3d9d26be5a29d8df1aa   \n1  fa1f648451baf909683ea40bda3a06fe   \n2  ae2c43f1062ab1dae0ad51f5ea3c7c56   \n3  3eb21a560e2afb02ace9e44d6fe76c8b   \n4  6caffed66bddb57550e777f04823fdd6   \n\n                                         review_text  \\\n0  4.5 stars \\n I loved this book! It was incredi...   \n1  I've had my dad and friend after me to read th...   \n2  Anyone desiring to start an exploration of S&M...   \n3  3.75 stars \\n Mal is the best. He's crazy fun ...   \n4  I would like to begin by saying how much I app...   \n\n                       date_added                    date_updated  \\\n0  Fri May 23 13:53:31 -0700 2014  Sun Apr 03 03:56:27 -0700 2016   \n1  Thu Jan 05 21:23:23 -0800 2017  Sun Jan 08 21:09:19 -0800 2017   \n2  Tue Feb 18 03:29:28 -0800 2014  Fri Feb 28 01:24:54 -0800 2014   \n3  Fri May 08 17:06:08 -0700 2015  Sun May 21 17:12:59 -0700 2017   \n4  Tue Feb 08 15:47:53 -0800 2011  Sat Nov 05 21:12:58 -0700 2011   \n\n                          read_at                      started_at  n_votes  \\\n0  Sun Mar 29 00:00:00 -0700 2015  Mon Aug 25 00:00:00 -0700 2014        0   \n1  Sun Jan 08 00:00:00 -0800 2017  Thu Jan 05 00:00:00 -0800 2017        1   \n2  Fri Feb 28 01:54:07 -0800 2014  Tue Feb 18 00:00:00 -0800 2014        1   \n3  Mon Sep 07 00:00:00 -0700 2015  Sun Sep 06 00:00:00 -0700 2015        1   \n4  Sat Nov 05 00:00:00 -0700 2011  Sun Oct 30 00:00:00 -0700 2011        0   \n\n   n_comments  rating  \n0           0       4  \n1           0       4  \n2           1       3  \n3           0       4  \n4           0       5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>review_text</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>e8730ce3ed5e9762038d160e23d47d79</td>\n      <td>2187</td>\n      <td>1c6af3913167e3d9d26be5a29d8df1aa</td>\n      <td>4.5 stars \\n I loved this book! It was incredi...</td>\n      <td>Fri May 23 13:53:31 -0700 2014</td>\n      <td>Sun Apr 03 03:56:27 -0700 2016</td>\n      <td>Sun Mar 29 00:00:00 -0700 2015</td>\n      <td>Mon Aug 25 00:00:00 -0700 2014</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26317e667d9141ad245dd2c18be52d77</td>\n      <td>47212</td>\n      <td>fa1f648451baf909683ea40bda3a06fe</td>\n      <td>I've had my dad and friend after me to read th...</td>\n      <td>Thu Jan 05 21:23:23 -0800 2017</td>\n      <td>Sun Jan 08 21:09:19 -0800 2017</td>\n      <td>Sun Jan 08 00:00:00 -0800 2017</td>\n      <td>Thu Jan 05 00:00:00 -0800 2017</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bf9640b81a047ee70b4f918082492f1d</td>\n      <td>40483</td>\n      <td>ae2c43f1062ab1dae0ad51f5ea3c7c56</td>\n      <td>Anyone desiring to start an exploration of S&amp;M...</td>\n      <td>Tue Feb 18 03:29:28 -0800 2014</td>\n      <td>Fri Feb 28 01:24:54 -0800 2014</td>\n      <td>Fri Feb 28 01:54:07 -0800 2014</td>\n      <td>Tue Feb 18 00:00:00 -0800 2014</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>34aa99d428ad98679c3e45d117243f55</td>\n      <td>19095025</td>\n      <td>3eb21a560e2afb02ace9e44d6fe76c8b</td>\n      <td>3.75 stars \\n Mal is the best. He's crazy fun ...</td>\n      <td>Fri May 08 17:06:08 -0700 2015</td>\n      <td>Sun May 21 17:12:59 -0700 2017</td>\n      <td>Mon Sep 07 00:00:00 -0700 2015</td>\n      <td>Sun Sep 06 00:00:00 -0700 2015</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6b3f929609c9d97628807d13b59b0b22</td>\n      <td>9464746</td>\n      <td>6caffed66bddb57550e777f04823fdd6</td>\n      <td>I would like to begin by saying how much I app...</td>\n      <td>Tue Feb 08 15:47:53 -0800 2011</td>\n      <td>Sat Nov 05 21:12:58 -0700 2011</td>\n      <td>Sat Nov 05 00:00:00 -0700 2011</td>\n      <td>Sun Oct 30 00:00:00 -0700 2011</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":3},{"cell_type":"markdown","source":"### 2. PreProcessing\nData Preprocessing for NLP Model:\nIn preparing the text data for our NLP model, following preprocessing functions are performed:\n \n* Lowercasing: The text has been converted to lowercase to ensure uniformity and eliminate case sensitivity.\n\n* HTML Tags and URLs Removal: HTML tags and URLs have been removed from the text to focus on meaningful content.\n\n* Punctuation Cleaning: Punctuation marks have been removed from the text strings to streamline analysis and avoid interference with the model.\n\n* Stopword Removal: Common stopwords, such as \"and,\" \"the,\" etc., have been eliminated to reduce noise and enhance the importance of significant words.\n\n* Tokenization and Stemming: The text has been tokenized into individual words or tokens, and a stemming process has been applied to reduce words to their root form. This aids in standardization and consolidates related words.","metadata":{}},{"cell_type":"code","source":"new_df = df\nnew_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:16:12.761292Z","iopub.execute_input":"2023-12-15T08:16:12.761644Z","iopub.status.idle":"2023-12-15T08:16:12.779033Z","shell.execute_reply.started":"2023-12-15T08:16:12.761610Z","shell.execute_reply":"2023-12-15T08:16:12.777987Z"},"trusted":true},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                            user_id   book_id  \\\n0  e8730ce3ed5e9762038d160e23d47d79      2187   \n1  26317e667d9141ad245dd2c18be52d77     47212   \n2  bf9640b81a047ee70b4f918082492f1d     40483   \n3  34aa99d428ad98679c3e45d117243f55  19095025   \n4  6b3f929609c9d97628807d13b59b0b22   9464746   \n\n                          review_id  \\\n0  1c6af3913167e3d9d26be5a29d8df1aa   \n1  fa1f648451baf909683ea40bda3a06fe   \n2  ae2c43f1062ab1dae0ad51f5ea3c7c56   \n3  3eb21a560e2afb02ace9e44d6fe76c8b   \n4  6caffed66bddb57550e777f04823fdd6   \n\n                                         review_text  \\\n0  4.5 stars \\n I loved this book! It was incredi...   \n1  I've had my dad and friend after me to read th...   \n2  Anyone desiring to start an exploration of S&M...   \n3  3.75 stars \\n Mal is the best. He's crazy fun ...   \n4  I would like to begin by saying how much I app...   \n\n                       date_added                    date_updated  \\\n0  Fri May 23 13:53:31 -0700 2014  Sun Apr 03 03:56:27 -0700 2016   \n1  Thu Jan 05 21:23:23 -0800 2017  Sun Jan 08 21:09:19 -0800 2017   \n2  Tue Feb 18 03:29:28 -0800 2014  Fri Feb 28 01:24:54 -0800 2014   \n3  Fri May 08 17:06:08 -0700 2015  Sun May 21 17:12:59 -0700 2017   \n4  Tue Feb 08 15:47:53 -0800 2011  Sat Nov 05 21:12:58 -0700 2011   \n\n                          read_at                      started_at  n_votes  \\\n0  Sun Mar 29 00:00:00 -0700 2015  Mon Aug 25 00:00:00 -0700 2014        0   \n1  Sun Jan 08 00:00:00 -0800 2017  Thu Jan 05 00:00:00 -0800 2017        1   \n2  Fri Feb 28 01:54:07 -0800 2014  Tue Feb 18 00:00:00 -0800 2014        1   \n3  Mon Sep 07 00:00:00 -0700 2015  Sun Sep 06 00:00:00 -0700 2015        1   \n4  Sat Nov 05 00:00:00 -0700 2011  Sun Oct 30 00:00:00 -0700 2011        0   \n\n   n_comments  rating  \n0           0       4  \n1           0       4  \n2           1       3  \n3           0       4  \n4           0       5  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>book_id</th>\n      <th>review_id</th>\n      <th>review_text</th>\n      <th>date_added</th>\n      <th>date_updated</th>\n      <th>read_at</th>\n      <th>started_at</th>\n      <th>n_votes</th>\n      <th>n_comments</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>e8730ce3ed5e9762038d160e23d47d79</td>\n      <td>2187</td>\n      <td>1c6af3913167e3d9d26be5a29d8df1aa</td>\n      <td>4.5 stars \\n I loved this book! It was incredi...</td>\n      <td>Fri May 23 13:53:31 -0700 2014</td>\n      <td>Sun Apr 03 03:56:27 -0700 2016</td>\n      <td>Sun Mar 29 00:00:00 -0700 2015</td>\n      <td>Mon Aug 25 00:00:00 -0700 2014</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>26317e667d9141ad245dd2c18be52d77</td>\n      <td>47212</td>\n      <td>fa1f648451baf909683ea40bda3a06fe</td>\n      <td>I've had my dad and friend after me to read th...</td>\n      <td>Thu Jan 05 21:23:23 -0800 2017</td>\n      <td>Sun Jan 08 21:09:19 -0800 2017</td>\n      <td>Sun Jan 08 00:00:00 -0800 2017</td>\n      <td>Thu Jan 05 00:00:00 -0800 2017</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>bf9640b81a047ee70b4f918082492f1d</td>\n      <td>40483</td>\n      <td>ae2c43f1062ab1dae0ad51f5ea3c7c56</td>\n      <td>Anyone desiring to start an exploration of S&amp;M...</td>\n      <td>Tue Feb 18 03:29:28 -0800 2014</td>\n      <td>Fri Feb 28 01:24:54 -0800 2014</td>\n      <td>Fri Feb 28 01:54:07 -0800 2014</td>\n      <td>Tue Feb 18 00:00:00 -0800 2014</td>\n      <td>1</td>\n      <td>1</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>34aa99d428ad98679c3e45d117243f55</td>\n      <td>19095025</td>\n      <td>3eb21a560e2afb02ace9e44d6fe76c8b</td>\n      <td>3.75 stars \\n Mal is the best. He's crazy fun ...</td>\n      <td>Fri May 08 17:06:08 -0700 2015</td>\n      <td>Sun May 21 17:12:59 -0700 2017</td>\n      <td>Mon Sep 07 00:00:00 -0700 2015</td>\n      <td>Sun Sep 06 00:00:00 -0700 2015</td>\n      <td>1</td>\n      <td>0</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>6b3f929609c9d97628807d13b59b0b22</td>\n      <td>9464746</td>\n      <td>6caffed66bddb57550e777f04823fdd6</td>\n      <td>I would like to begin by saying how much I app...</td>\n      <td>Tue Feb 08 15:47:53 -0800 2011</td>\n      <td>Sat Nov 05 21:12:58 -0700 2011</td>\n      <td>Sat Nov 05 00:00:00 -0700 2011</td>\n      <td>Sun Oct 30 00:00:00 -0700 2011</td>\n      <td>0</td>\n      <td>0</td>\n      <td>5</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"### HTML Tag and urls removal","metadata":{}},{"cell_type":"markdown","source":"* In this function we have used the regular expressions to remove html tags, urls, tabs and new line characters. ","metadata":{}},{"cell_type":"code","source":"import re\n\ndef remove_html_tags_and_urls(text):\n    clean = re.sub(r'[\\t\\n]', '', text)\n    clean = re.sub(r'<.*?>', '', clean)\n    clean = re.sub(r'https?://\\S+|www\\.\\S+', '', clean)\n    return clean","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:16:12.780406Z","iopub.execute_input":"2023-12-15T08:16:12.780732Z","iopub.status.idle":"2023-12-15T08:16:12.788220Z","shell.execute_reply.started":"2023-12-15T08:16:12.780690Z","shell.execute_reply":"2023-12-15T08:16:12.787308Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Remove Punctuation","metadata":{}},{"cell_type":"markdown","source":"* We have remove the punctuation except the points in the integers to ensure that there is no loss of information","metadata":{}},{"cell_type":"code","source":"import string\nstring.punctuation","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:16:12.789337Z","iopub.execute_input":"2023-12-15T08:16:12.789579Z","iopub.status.idle":"2023-12-15T08:16:12.800073Z","shell.execute_reply.started":"2023-12-15T08:16:12.789556Z","shell.execute_reply":"2023-12-15T08:16:12.799202Z"},"trusted":true},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"st = '''!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'''","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:16:12.801131Z","iopub.execute_input":"2023-12-15T08:16:12.801412Z","iopub.status.idle":"2023-12-15T08:16:12.809381Z","shell.execute_reply.started":"2023-12-15T08:16:12.801387Z","shell.execute_reply":"2023-12-15T08:16:12.808486Z"},"trusted":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\ndef remove_punc(text):\n    return text.translate(str.maketrans('','', st))","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:16:12.810904Z","iopub.execute_input":"2023-12-15T08:16:12.811211Z","iopub.status.idle":"2023-12-15T08:16:12.819762Z","shell.execute_reply.started":"2023-12-15T08:16:12.811180Z","shell.execute_reply":"2023-12-15T08:16:12.818819Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### Clean String","metadata":{}},{"cell_type":"code","source":"def clean_string(input_str):\n    cleaned_str = ''.join(char for char in input_str if char.isalnum() or char.isspace() or char == '.')\n    return cleaned_str","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:16:12.821189Z","iopub.execute_input":"2023-12-15T08:16:12.821502Z","iopub.status.idle":"2023-12-15T08:16:12.831360Z","shell.execute_reply.started":"2023-12-15T08:16:12.821472Z","shell.execute_reply":"2023-12-15T08:16:12.830435Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### Except Dots","metadata":{}},{"cell_type":"code","source":"import re\n\ndef remove_dots_except_decimals(input_str):\n    result = re.sub(r'(?<!\\d)\\.(?!\\d)', '', input_str)\n    result = re.sub(r'\\.(?!\\d)', '', result)\n    return result","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:16:12.835817Z","iopub.execute_input":"2023-12-15T08:16:12.836096Z","iopub.status.idle":"2023-12-15T08:16:12.842240Z","shell.execute_reply.started":"2023-12-15T08:16:12.836073Z","shell.execute_reply":"2023-12-15T08:16:12.841356Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Stopwords Removal","metadata":{}},{"cell_type":"markdown","source":"* Here we have used the NLTK library's stopwords to remove the stopwords from the reviews to focus on the significant words more.","metadata":{}},{"cell_type":"code","source":"from nltk.corpus import stopwords\n\nstop_words = set(stopwords.words('english'))\n\ndef remove_stopword(text):\n    words = text.split()\n    filtered_words = [word for word in words if word not in stop_words]\n    return ' '.join(filtered_words)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:16:12.843254Z","iopub.execute_input":"2023-12-15T08:16:12.843782Z","iopub.status.idle":"2023-12-15T08:16:13.982845Z","shell.execute_reply.started":"2023-12-15T08:16:12.843756Z","shell.execute_reply":"2023-12-15T08:16:13.981838Z"},"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"### Tokenization and Stemming","metadata":{}},{"cell_type":"markdown","source":"* Two-way stemming condenses words to their roots and retains potential reversibility, enhancing linguistic context preservation, aiding interpretability, and ensuring readability in the processed text.","metadata":{}},{"cell_type":"code","source":"from nltk.stem import PorterStemmer\nclass TwoWayStemmer:\n    def __init__(self):\n        self.stemmer = PorterStemmer()\n        self.stem_dict = {}\n\n    def stem_text(self, text):\n        stemmed_text = []\n        for word in text.split():\n            stemmed_word = self.stemmer.stem(word)\n            if stemmed_word not in self.stem_dict:\n                self.stem_dict[stemmed_word] = [word]\n            else:\n                if word not in self.stem_dict[stemmed_word]:\n                    self.stem_dict[stemmed_word].append(word)\n            stemmed_text.append(stemmed_word)\n        return ' '.join(stemmed_text)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:16:13.991490Z","iopub.execute_input":"2023-12-15T08:16:13.991782Z","iopub.status.idle":"2023-12-15T08:16:14.001283Z","shell.execute_reply.started":"2023-12-15T08:16:13.991757Z","shell.execute_reply":"2023-12-15T08:16:14.000483Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"### Applying Functions","metadata":{}},{"cell_type":"code","source":"new_df['review_text'] = new_df['review_text'].str.lower()\nnew_df['review_text'].head()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:16:14.002441Z","iopub.execute_input":"2023-12-15T08:16:14.002704Z","iopub.status.idle":"2023-12-15T08:16:15.115149Z","shell.execute_reply.started":"2023-12-15T08:16:14.002680Z","shell.execute_reply":"2023-12-15T08:16:15.114257Z"},"trusted":true},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"0    4.5 stars \\n i loved this book! it was incredi...\n1    i've had my dad and friend after me to read th...\n2    anyone desiring to start an exploration of s&m...\n3    3.75 stars \\n mal is the best. he's crazy fun ...\n4    i would like to begin by saying how much i app...\nName: review_text, dtype: object"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"new_df['review_text'] = new_df['review_text'].apply(remove_html_tags_and_urls)\nnew_df['review_text'].head()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:16:15.116138Z","iopub.execute_input":"2023-12-15T08:16:15.116389Z","iopub.status.idle":"2023-12-15T08:16:49.253779Z","shell.execute_reply.started":"2023-12-15T08:16:15.116366Z","shell.execute_reply":"2023-12-15T08:16:49.252862Z"},"trusted":true},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"0    4.5 stars  i loved this book! it was incredibl...\n1    i've had my dad and friend after me to read th...\n2    anyone desiring to start an exploration of s&m...\n3    3.75 stars  mal is the best. he's crazy fun an...\n4    i would like to begin by saying how much i app...\nName: review_text, dtype: object"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"new_df['review_text'] = new_df['review_text'].apply(clean_string)\nnew_df['review_text'].head()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:16:49.254965Z","iopub.execute_input":"2023-12-15T08:16:49.255272Z","iopub.status.idle":"2023-12-15T08:18:30.127391Z","shell.execute_reply.started":"2023-12-15T08:16:49.255246Z","shell.execute_reply":"2023-12-15T08:18:30.126484Z"},"trusted":true},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"0    4.5 stars  i loved this book it was incredibly...\n1    ive had my dad and friend after me to read thi...\n2    anyone desiring to start an exploration of sm ...\n3    3.75 stars  mal is the best. hes crazy fun and...\n4    i would like to begin by saying how much i app...\nName: review_text, dtype: object"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"new_df['review_text'] = new_df['review_text'].apply(remove_dots_except_decimals)\nnew_df['review_text'].head()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:18:30.128817Z","iopub.execute_input":"2023-12-15T08:18:30.129741Z","iopub.status.idle":"2023-12-15T08:19:26.523401Z","shell.execute_reply.started":"2023-12-15T08:18:30.129713Z","shell.execute_reply":"2023-12-15T08:19:26.522452Z"},"trusted":true},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"0    4.5 stars  i loved this book it was incredibly...\n1    ive had my dad and friend after me to read thi...\n2    anyone desiring to start an exploration of sm ...\n3    3.75 stars  mal is the best hes crazy fun and ...\n4    i would like to begin by saying how much i app...\nName: review_text, dtype: object"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"new_df['review_text'] = new_df['review_text'].apply(remove_stopword)\nnew_df['review_text'].head()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:19:26.524603Z","iopub.execute_input":"2023-12-15T08:19:26.524917Z","iopub.status.idle":"2023-12-15T08:19:48.684410Z","shell.execute_reply.started":"2023-12-15T08:19:26.524890Z","shell.execute_reply":"2023-12-15T08:19:48.683518Z"},"trusted":true},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"0    4.5 stars loved book incredibly intricate well...\n1    ive dad friend read series years friend gave g...\n2    anyone desiring start exploration sm erotica s...\n3    3.75 stars mal best hes crazy fun says things ...\n4    would like begin saying much appreciate amount...\nName: review_text, dtype: object"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"two_way_stemmer = TwoWayStemmer()\n\n# Assuming 'train_df' is your training dataframe with 'review_text' column\nnew_df['review_text'] = new_df['review_text'].apply(lambda x: two_way_stemmer.stem_text(x))","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:19:48.685363Z","iopub.execute_input":"2023-12-15T08:19:48.685621Z","iopub.status.idle":"2023-12-15T08:54:03.816965Z","shell.execute_reply.started":"2023-12-15T08:19:48.685598Z","shell.execute_reply":"2023-12-15T08:54:03.816115Z"},"trusted":true},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"### MODEL TRAINING\n\n* It imports necessary modules for text and categorical feature processing, including TfidfVectorizer for text, OneHotEncoder for categorical features.\n* It sets up a ColumnTransformer to preprocess text data with TfidfVectorizer (5000 features, word and bi-gram range) and one-hot encodes a user ID feature.\n* The neural network architecture isn't defined here but can concatenate processed inputs using Keras layers for later model building.\n* train_test_split might be used later for data splitting. Overall, it's setting up a pipeline for feature preprocessing for a neural network model.\n","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n\nX_train, X_test, y_train, y_test = train_test_split(\n    new_df[['user_id','review_text']], new_df['rating'], test_size=0.2, random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T09:55:35.906100Z","iopub.execute_input":"2023-12-15T09:55:35.907014Z","iopub.status.idle":"2023-12-15T09:55:36.189056Z","shell.execute_reply.started":"2023-12-15T09:55:35.906974Z","shell.execute_reply":"2023-12-15T09:55:36.187871Z"},"trusted":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"text_features = 'review_text'\nuser_id_feature = 'user_id'\nvotes_feature = 'n_votes'\ncomments_feature = 'n_comments'","metadata":{"execution":{"iopub.status.busy":"2023-12-15T09:55:40.133006Z","iopub.execute_input":"2023-12-15T09:55:40.133983Z","iopub.status.idle":"2023-12-15T09:55:40.138684Z","shell.execute_reply.started":"2023-12-15T09:55:40.133945Z","shell.execute_reply":"2023-12-15T09:55:40.137319Z"},"trusted":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom keras.layers import Input, Dense, Concatenate\nfrom keras.models import Model\nimport numpy as np\n\n\n# Preprocessing\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('text', TfidfVectorizer(max_features=5000, ngram_range=(1, 2)), text_features),\n        ('user_id', OneHotEncoder(handle_unknown='ignore'), [user_id_feature])  \n    ]\n)\n\n\n# Fit and transform the data using the preprocessor\nX_processed = preprocessor.fit_transform(X_train)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T10:58:03.683555Z","iopub.execute_input":"2023-12-15T10:58:03.683988Z","iopub.status.idle":"2023-12-15T11:01:07.621521Z","shell.execute_reply.started":"2023-12-15T10:58:03.683954Z","shell.execute_reply":"2023-12-15T11:01:07.620596Z"},"trusted":true},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# Split the data\nX_train_tfidf, X_val_tfidf, y_train_tfidf, y_val = train_test_split(\n    X_processed, y_train, test_size=0.1, random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T11:01:07.623387Z","iopub.execute_input":"2023-12-15T11:01:07.623674Z","iopub.status.idle":"2023-12-15T11:01:07.852018Z","shell.execute_reply.started":"2023-12-15T11:01:07.623648Z","shell.execute_reply":"2023-12-15T11:01:07.851208Z"},"trusted":true},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":"### Creating Custom Neural Network","metadata":{}},{"cell_type":"markdown","source":"* The provided code sets up a neural network using the Keras library. It constructs a Sequential model comprising dense layers, gradually reducing neurons. Leaky Rectified Linear Unit (LeakyReLU) activation functions introduce non-linearity in each layer. The output layer has six neurons for multi-class classification, utilizing softmax activation to output class probabilities. Additionally, it compiles the model, configuring it for training with the Adam optimizer (learning rate 0.001), categorical cross-entropy loss for multiclass problems, and accuracy as the evaluation metric. Overall, it builds a neural network architecture, ready for training on data with defined input dimensions and classification into multiple classes.","metadata":{}},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, LeakyReLU\nfrom keras.optimizers import Adam\n\n# Create a Sequential model\nmodel = Sequential()\n\n# Add layers to the Sequential model\n\nmodel.add(Dense(512, input_shape=(X_train_tfidf.shape[1],)))\nmodel.add(LeakyReLU(alpha=0.2))\n\nmodel.add(Dense(256))\nmodel.add(LeakyReLU(alpha=0.2))\n\nmodel.add(Dense(128))\nmodel.add(LeakyReLU(alpha=0.2))\n\nmodel.add(Dense(64))\nmodel.add(LeakyReLU(alpha=0.2))\n\nmodel.add(Dense(6, activation='softmax'))\n\n# Compile the model\ncustom_optimizer = Adam(learning_rate=0.001)  \nmodel.compile(optimizer=custom_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:04:15.433183Z","iopub.execute_input":"2023-12-15T12:04:15.433564Z","iopub.status.idle":"2023-12-15T12:04:15.610310Z","shell.execute_reply.started":"2023-12-15T12:04:15.433532Z","shell.execute_reply":"2023-12-15T12:04:15.609518Z"},"trusted":true},"outputs":[],"execution_count":68},{"cell_type":"code","source":"from keras.utils import to_categorical\n\n# Assuming y_train contains class indices\ny_train_encoded = to_categorical(y_train_tfidf)\ny_val_encoded = to_categorical(y_val)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T11:20:23.218573Z","iopub.execute_input":"2023-12-15T11:20:23.218975Z","iopub.status.idle":"2023-12-15T11:20:23.231778Z","shell.execute_reply.started":"2023-12-15T11:20:23.218941Z","shell.execute_reply":"2023-12-15T11:20:23.230828Z"},"trusted":true},"outputs":[],"execution_count":56},{"cell_type":"code","source":"# Train the model\nhistory = model.fit(\n    X_train_tfidf, y_train_encoded, \n    epochs=2, batch_size=32, \n    validation_data=(X_val_tfidf, y_val_encoded)\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:04:19.498771Z","iopub.execute_input":"2023-12-15T12:04:19.499686Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Epoch 1/2\n28350/28350 [==============================] - 408s 14ms/step - loss: 1.0499 - accuracy: 0.5517 - val_loss: 1.0016 - val_accuracy: 0.5744\nEpoch 2/2\n 6971/28350 [======>.......................] - ETA: 3:17 - loss: 0.9325 - accuracy: 0.6062","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"X_test_processed = preprocessor.transform(X_test)\ny_test_encoded = to_categorical(y_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T11:41:33.092285Z","iopub.execute_input":"2023-12-15T11:41:33.092737Z","iopub.status.idle":"2023-12-15T11:42:01.855219Z","shell.execute_reply.started":"2023-12-15T11:41:33.092702Z","shell.execute_reply":"2023-12-15T11:42:01.854378Z"},"trusted":true},"outputs":[],"execution_count":58},{"cell_type":"code","source":"test_loss, test_accuracy = model.evaluate(X_test_processed, y_test_encoded)\n\nprint(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\nprint(f\"Test Loss: {test_loss}\")","metadata":{"execution":{"iopub.status.busy":"2023-12-15T11:42:01.856862Z","iopub.execute_input":"2023-12-15T11:42:01.857159Z","iopub.status.idle":"2023-12-15T11:42:16.813175Z","shell.execute_reply.started":"2023-12-15T11:42:01.857134Z","shell.execute_reply":"2023-12-15T11:42:16.812159Z"},"trusted":true},"outputs":[{"name":"stdout","text":"3938/3938 [==============================] - 14s 4ms/step - loss: 1.0697 - accuracy: 0.5685\nTest Accuracy: 56.85%\nTest Loss: 1.0697451829910278\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"### Predicting TEST DATA","metadata":{}},{"cell_type":"markdown","source":"* For prediction the test data has to be preprocessed as the training data was done. So applying the same preprocessing steps on the testing data and transforming it using the same column transform for prediction","metadata":{}},{"cell_type":"code","source":"test_df = pd.read_csv(\"/kaggle/input/book-rating-prediction01/Test.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-12-15T11:42:16.815160Z","iopub.execute_input":"2023-12-15T11:42:16.815441Z","iopub.status.idle":"2023-12-15T11:42:23.786196Z","shell.execute_reply.started":"2023-12-15T11:42:16.815416Z","shell.execute_reply":"2023-12-15T11:42:23.785345Z"},"trusted":true},"outputs":[],"execution_count":60},{"cell_type":"code","source":"# Applying functions\ntest_df['review_text'] = test_df['review_text'].str.lower()\ntest_df['review_text'] = test_df['review_text'].apply(remove_html_tags_and_urls)\ntest_df['review_text'] = test_df['review_text'].apply(clean_string)\ntest_df['review_text'] = test_df['review_text'].apply(remove_dots_except_decimals)\ntest_df['review_text'] = test_df['review_text'].apply(remove_stopword)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T11:42:23.788136Z","iopub.execute_input":"2023-12-15T11:42:23.788441Z","iopub.status.idle":"2023-12-15T11:43:55.207935Z","shell.execute_reply.started":"2023-12-15T11:42:23.788414Z","shell.execute_reply":"2023-12-15T11:43:55.206827Z"},"trusted":true},"outputs":[],"execution_count":61},{"cell_type":"code","source":"# Assuming 'train_df' is your training dataframe with 'review_text' column\ntest_df['review_text'] = test_df['review_text'].apply(lambda x: two_way_stemmer.stem_text(x))","metadata":{"execution":{"iopub.status.busy":"2023-12-15T11:43:55.209477Z","iopub.execute_input":"2023-12-15T11:43:55.209760Z","iopub.status.idle":"2023-12-15T11:58:32.823304Z","shell.execute_reply.started":"2023-12-15T11:43:55.209736Z","shell.execute_reply":"2023-12-15T11:58:32.822145Z"},"trusted":true},"outputs":[],"execution_count":62},{"cell_type":"code","source":"test_processed = preprocessor.transform(test_df[['user_id','review_text']])","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:02:07.694477Z","iopub.execute_input":"2023-12-15T12:02:07.695311Z","iopub.status.idle":"2023-12-15T12:03:08.524545Z","shell.execute_reply.started":"2023-12-15T12:02:07.695280Z","shell.execute_reply":"2023-12-15T12:03:08.523611Z"},"trusted":true},"outputs":[],"execution_count":66},{"cell_type":"code","source":"predictions = model.predict(test_processed)\npredicted_classes = np.argmax(predictions, axis=1)\npredicted_classes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Assuming test_df['review_id'] and predictions are two-dimensional arrays of shape (270000, 1)\npredictions_df = pd.DataFrame({'review_id': test_df['review_id'], 'rating': predicted_classes})\n\n# Save the DataFrame to a CSV file\npredictions_df.to_csv('predictions_final.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-15T12:00:05.764478Z","iopub.execute_input":"2023-12-15T12:00:05.765175Z","iopub.status.idle":"2023-12-15T12:00:06.620647Z","shell.execute_reply.started":"2023-12-15T12:00:05.765136Z","shell.execute_reply":"2023-12-15T12:00:06.619829Z"},"trusted":true},"outputs":[],"execution_count":65},{"cell_type":"code","source":"predictions_df['rating'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-12-15T08:57:10.806011Z","iopub.status.idle":"2023-12-15T08:57:10.806410Z","shell.execute_reply.started":"2023-12-15T08:57:10.806238Z","shell.execute_reply":"2023-12-15T08:57:10.806255Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}